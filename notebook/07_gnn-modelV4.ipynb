{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1109ea72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "\n",
    "# GNN / PyG\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GATv2Conv\n",
    "\n",
    "# Sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "# Torch utils\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "# Project imports (config + logger)\n",
    "sys.path.append(os.path.abspath('../src'))\n",
    "import config\n",
    "from utils import setup_logger\n",
    "\n",
    "sns.set_style('whitegrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4788e1a3",
   "metadata": {},
   "source": [
    "# GNN Model V4 — Temporal + Spatial Learning (GATv2Conv)\n",
    "\n",
    "This notebook implements a stronger iteration of our GNN by adding **temporal memory** on top of the spatial stop-graph model.\n",
    "\n",
    "## What changes vs Model V2\n",
    "- **Model V2**: Spatial stop embeddings (GAT) + per-row context (lag + time + sequence index).\n",
    "- **Model V4**: Spatial stop embeddings (**GATv2Conv**) + **windowed temporal encoder (GRU)** over each trip’s recent context history.\n",
    "\n",
    "## Why this can improve MAE\n",
    "Transit delay is dynamic: being late at the previous stop and how that lateness evolves across the last few stops are strong predictors of the next delay. A GRU can encode this short-term evolution more directly than a single lag value.\n",
    "\n",
    "All hyperparameters and paths are loaded from `src/config.py` (with `GNN_V4_*` keys)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "928767f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-13 22:14:45,341 - INFO - Logging initialized. Saving logs to: /home/fustli/Documents/Uni/DeepL/End-of-trip_delay_prediction/log/gnn_model_v4_training.log\n",
      "2025-12-13 22:14:45,345 - INFO - --- STARTING GNN MODEL V4 TRAINING ---\n",
      "2025-12-13 22:14:45,346 - INFO - Device: cuda\n",
      "2025-12-13 22:14:45,364 - INFO - GPU: NVIDIA GeForce RTX 4060\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# STEP 0: RUNTIME SETUP (CONFIG + LOGGING)\n",
    "# =============================================================================\n",
    "\n",
    "log_dir = getattr(config, 'LOG_DIR', 'log')\n",
    "logger = setup_logger(\n",
    "    name='gnn_model_v4',\n",
    "    log_dir=log_dir,\n",
    "    filename='gnn_model_v4_training.log',\n",
    "    level=logging.INFO,\n",
    "    mode='w',\n",
    ")\n",
    "\n",
    "seed = int(getattr(config, 'SEED', 42))\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "logger.info('--- STARTING GNN MODEL V4 TRAINING ---')\n",
    "logger.info(f\"Device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    logger.info(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "plots_dir = getattr(config, 'PLOTS_DIR', os.path.join(os.getcwd(), 'plots'))\n",
    "os.makedirs(plots_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1a53668",
   "metadata": {},
   "source": [
    "## 1. Data Loading + Canonical Sorting\n",
    "\n",
    "We load the cleaned dataset and enforce a strict sort by `(trip_id, timestamp)` so that temporal features (lags, deltas, rolling windows) are correct and reproducible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a5393488",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-13 22:14:48,079 - INFO - Loading data from: /home/fustli/Documents/Uni/DeepL/End-of-trip_delay_prediction/data/vehicle_positions_cleaned.csv\n",
      "2025-12-13 22:15:16,556 - INFO - Rows after basic cleanup: 15,065,416\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# STEP 1: LOAD DATA\n",
    "# =============================================================================\n",
    "\n",
    "clean_path = getattr(\n",
    "    config,\n",
    "    'CLEANED_CSV_PATH',\n",
    "    os.path.join(getattr(config, 'DATA_DIR', 'data'), 'vehicle_positions_cleaned.csv')\n",
    ")\n",
    "logger.info(f\"Loading data from: {clean_path}\")\n",
    "df = pd.read_csv(clean_path)\n",
    "\n",
    "required_cols = ['timestamp', 'trip_id', 'delay_seconds', 'latitude', 'longitude']\n",
    "missing_required = [c for c in required_cols if c not in df.columns]\n",
    "if missing_required:\n",
    "    raise ValueError(f\"Missing required columns: {missing_required}\")\n",
    "\n",
    "# Choose stop column name consistently\n",
    "stop_col = 'last_stop_id' if 'last_stop_id' in df.columns else 'stop_id'\n",
    "if stop_col not in df.columns:\n",
    "    raise ValueError(\"No stop column found (expected 'last_stop_id' or 'stop_id')\")\n",
    "\n",
    "# Basic cleanup (keep minimal; do not silently change semantics)\n",
    "df = df.dropna(subset=['timestamp', 'trip_id', stop_col, 'delay_seconds', 'latitude', 'longitude']).copy()\n",
    "df['dt'] = pd.to_datetime(df['timestamp'], errors='coerce')\n",
    "df = df.dropna(subset=['dt']).copy()\n",
    "\n",
    "# Canonical sort for temporal feature engineering\n",
    "df = df.sort_values(['trip_id', 'dt']).reset_index(drop=True)\n",
    "logger.info(f\"Rows after basic cleanup: {len(df):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b74cf4",
   "metadata": {},
   "source": [
    "## 2. Feature Engineering (V4: Temporal-Signal Upgrade)\n",
    "\n",
    "Model V4 uses the same temporal features as the previous model (so the spatial layer change is isolated):\n",
    "\n",
    "- **Lag**: previous stop delay (`prev_stop_delay`).\n",
    "- **Rolling lag**: short-window mean of lag (`rolling_prev_delay`) to stabilize noise.\n",
    "- **Time delta**: seconds since the previous observation in the trip (`time_delta_sec`).\n",
    "- **Progress**: normalized trip progress $\\in [0,1]$ so the model can compare different trip lengths.\n",
    "- **Cyclical time**: sine/cosine for hour and day-of-week.\n",
    "\n",
    "We then scale static node features and dynamic context features for stable training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "07a3e2e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-13 22:15:31,673 - INFO - --- Generating V4 Features ---\n",
      "2025-12-13 22:15:42,320 - INFO - Scaling features...\n",
      "2025-12-13 22:15:43,518 - INFO - Feature engineering complete.\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# STEP 2: FEATURE ENGINEERING (V4)\n",
    "# =============================================================================\n",
    "\n",
    "logger.info('--- Generating V4 Features ---')\n",
    "\n",
    "# -------------------------\n",
    "# Config-driven knobs\n",
    "# -------------------------\n",
    "lag_clip_min = int(getattr(config, 'GNN_V4_LAG_CLIP_MIN', -1800))\n",
    "lag_clip_max = int(getattr(config, 'GNN_V4_LAG_CLIP_MAX', 3600))\n",
    "time_delta_clip = int(getattr(config, 'GNN_V4_TIME_DELTA_CLIP_SEC', 900))\n",
    "rolling_w = int(getattr(config, 'GNN_V4_ROLLING_LAG_WINDOW', 3))\n",
    "\n",
    "# 1) Lag feature\n",
    "df['prev_stop_delay'] = df.groupby('trip_id')['delay_seconds'].shift(1).fillna(0.0)\n",
    "\n",
    "# 2) Rolling lag mean (using only past information)\n",
    "df['rolling_prev_delay'] = (\n",
    "    df.groupby('trip_id')['prev_stop_delay']\n",
    "      .rolling(window=rolling_w, min_periods=1)\n",
    "      .mean()\n",
    "      .reset_index(level=0, drop=True)\n",
    ")\n",
    "\n",
    "# 3) Time delta between consecutive observations within trip\n",
    "df['time_delta_sec'] = df.groupby('trip_id')['dt'].diff().dt.total_seconds().fillna(0.0)\n",
    "df['time_delta_sec'] = df['time_delta_sec'].clip(0, time_delta_clip)\n",
    "\n",
    "# 4) Progress in trip (normalized)\n",
    "trip_len = df.groupby('trip_id')[stop_col].transform('size').astype(np.float32)\n",
    "stop_sequence = df.groupby('trip_id').cumcount().astype(np.float32)\n",
    "df['progress'] = np.where(trip_len > 1, stop_sequence / (trip_len - 1.0), 0.0)\n",
    "\n",
    "# 5) Cyclical time embeddings\n",
    "df['hour'] = df['dt'].dt.hour\n",
    "df['day_of_week'] = df['dt'].dt.dayofweek\n",
    "df['hour_sin'] = np.sin(2 * np.pi * df['hour'] / 24)\n",
    "df['hour_cos'] = np.cos(2 * np.pi * df['hour'] / 24)\n",
    "df['day_sin'] = np.sin(2 * np.pi * df['day_of_week'] / 7)\n",
    "df['day_cos'] = np.cos(2 * np.pi * df['day_of_week'] / 7)\n",
    "\n",
    "# 6) Stop-level historical mean delay (static signal)\n",
    "global_mean = float(df['delay_seconds'].mean())\n",
    "stop_history = df.groupby(stop_col)['delay_seconds'].mean()\n",
    "df['history_mean'] = df[stop_col].map(stop_history).fillna(global_mean)\n",
    "\n",
    "# -------------------------\n",
    "# Scaling\n",
    "# -------------------------\n",
    "logger.info('Scaling features...')\n",
    "\n",
    "# A) Node features (static): lat/lon + history\n",
    "scaler_nodes = StandardScaler()\n",
    "df[['lat_scaled', 'lon_scaled', 'hist_scaled']] = scaler_nodes.fit_transform(\n",
    "    df[['latitude', 'longitude', 'history_mean']]\n",
    " )\n",
    "\n",
    "# B) Dynamic features (context stream)\n",
    "df['prev_delay_clipped'] = df['prev_stop_delay'].clip(lag_clip_min, lag_clip_max)\n",
    "df['rolling_prev_delay_clipped'] = df['rolling_prev_delay'].clip(lag_clip_min, lag_clip_max)\n",
    "\n",
    "scaler_lag = StandardScaler()\n",
    "df['prev_delay_scaled'] = scaler_lag.fit_transform(df[['prev_delay_clipped']])\n",
    "\n",
    "scaler_roll = StandardScaler()\n",
    "df['rolling_prev_delay_scaled'] = scaler_roll.fit_transform(df[['rolling_prev_delay_clipped']])\n",
    "\n",
    "scaler_delta = StandardScaler()\n",
    "df['time_delta_scaled'] = scaler_delta.fit_transform(df[['time_delta_sec']])\n",
    "\n",
    "scaler_prog = StandardScaler()\n",
    "df['progress_scaled'] = scaler_prog.fit_transform(df[['progress']])\n",
    "\n",
    "logger.info('Feature engineering complete.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed3b448a",
   "metadata": {},
   "source": [
    "## 3. Construct the Static Stop Graph\n",
    "\n",
    "We build a directed stop graph once and keep it on the GPU. Each training example will reference a stop index (node) plus a **temporal context window**.\n",
    "\n",
    "- **Nodes**: unique stops, with static features `[lat_scaled, lon_scaled, hist_scaled]`.\n",
    "- **Edges**: inferred from sequential stop transitions within trips (A → B if B follows A)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0836582f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-13 22:15:47,729 - INFO - --- Constructing Transit Graph ---\n",
      "2025-12-13 22:15:58,149 - INFO - Graph ready | nodes=5,212 edges=38,207\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# STEP 3: GRAPH CONSTRUCTION\n",
    "# =============================================================================\n",
    "\n",
    "logger.info('--- Constructing Transit Graph ---')\n",
    "\n",
    "stop_encoder = LabelEncoder()\n",
    "df['stop_idx'] = stop_encoder.fit_transform(df[stop_col].astype(str))\n",
    "\n",
    "# Node feature tensor (mean over rows that visit each stop)\n",
    "node_cols = ['lat_scaled', 'lon_scaled', 'hist_scaled']\n",
    "node_features_df = df.groupby('stop_idx')[node_cols].mean()\n",
    "x = torch.tensor(node_features_df.values, dtype=torch.float32)\n",
    "\n",
    "# Edges from sequential transitions within trip\n",
    "df_sorted = df.sort_values(by=['trip_id', 'dt']).copy()\n",
    "df_sorted['next_stop_idx'] = df_sorted.groupby('trip_id')['stop_idx'].shift(-1)\n",
    "edges_df = df_sorted.dropna(subset=['next_stop_idx'])\n",
    "unique_edges = edges_df[['stop_idx', 'next_stop_idx']].drop_duplicates()\n",
    "edge_index = torch.tensor(unique_edges.values.T, dtype=torch.long)\n",
    "\n",
    "graph_data = Data(x=x, edge_index=edge_index).to(device)\n",
    "logger.info(f\"Graph ready | nodes={graph_data.num_nodes:,} edges={graph_data.num_edges:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e6a6e45",
   "metadata": {},
   "source": [
    "## 4. Build a Windowed Temporal Dataset (Per-Trip Sequences)\n",
    "\n",
    "The key upgrade in Model V4 is that we do not feed only a single “previous delay” scalar.\n",
    "Instead, we create a short window of recent context for each row (within the same `trip_id`) and encode it with a GRU.\n",
    "\n",
    "**How the windows work**\n",
    "- For each row at position $t$ in a trip, we take the last $k$ timesteps of context features up to $t$ (left-padded with zeros if needed).\n",
    "- We also store the true window length (for packing into the GRU).\n",
    "- The static graph stays unchanged; each example also carries a `stop_idx` to pull the correct node embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "26af08bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-13 22:16:01,287 - INFO - --- Preparing lazy temporal windows for GRU ---\n",
      "2025-12-13 22:16:01,581 - INFO - Base arrays | rows=15,065,416 seq_len=12 features=8\n",
      "2025-12-13 22:16:04,976 - INFO - Split sizes | train=10,542,393 val=1,519,938 test=3,003,085 | split_by_trip=True\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# STEP 4: WINDOWED SEQUENCE DATASET (V4)\n",
    "# =============================================================================\n",
    "\n",
    "logger.info('--- Preparing lazy temporal windows for GRU ---')\n",
    "\n",
    "seq_len = int(getattr(config, 'GNN_V4_SEQ_LEN', 12))\n",
    "split_by_trip = bool(getattr(config, 'GNN_V4_SPLIT_BY_TRIP', True))\n",
    "\n",
    "# Dynamic context features to be fed as a temporal sequence\n",
    "context_seq_cols = [\n",
    "    'hour_sin', 'hour_cos', 'day_sin', 'day_cos',\n",
    "    'prev_delay_scaled', 'rolling_prev_delay_scaled',\n",
    "    'time_delta_scaled', 'progress_scaled',\n",
    "]\n",
    "missing_ctx = [c for c in context_seq_cols if c not in df.columns]\n",
    "if missing_ctx:\n",
    "    raise ValueError(f\"Missing context columns for V4: {missing_ctx}\")\n",
    "\n",
    "# IMPORTANT: With ~15M rows, pre-building X_seq_all would allocate multiple GB and can freeze VS Code.\n",
    "# Instead, we keep only the base arrays and build windows on-the-fly per batch via a custom collate_fn.\n",
    "\n",
    "X_ctx_base = df[context_seq_cols].to_numpy(dtype=np.float32, copy=False)\n",
    "stop_idx_base = df['stop_idx'].to_numpy(dtype=np.int64, copy=False)\n",
    "y_base = df['delay_seconds'].to_numpy(dtype=np.float32, copy=False).reshape(-1, 1)\n",
    "\n",
    "n = len(df)\n",
    "num_feat = len(context_seq_cols)\n",
    "logger.info(f\"Base arrays | rows={n:,} seq_len={seq_len} features={num_feat}\")\n",
    "\n",
    "# Precompute trip segment info (cheap vs building a (n, seq_len, feat) tensor)\n",
    "trip_codes, _ = pd.factorize(df['trip_id'].astype(str), sort=False)\n",
    "change = np.flatnonzero(np.diff(trip_codes) != 0) + 1\n",
    "starts = np.concatenate(([0], change))\n",
    "ends = np.concatenate((change, [n]))\n",
    "\n",
    "trip_start = np.empty((n,), dtype=np.int64)\n",
    "pos_in_trip = np.empty((n,), dtype=np.int64)\n",
    "for s, e in zip(starts, ends):\n",
    "    trip_start[s:e] = s\n",
    "    pos_in_trip[s:e] = np.arange(e - s, dtype=np.int64)\n",
    "\n",
    "# -------------------------\n",
    "# Train / Val / Test split\n",
    "# -------------------------\n",
    "random_state = int(getattr(config, 'RANDOM_STATE', getattr(config, 'SEED', 42)))\n",
    "test_size = float(getattr(config, 'TEST_SIZE', 0.2))\n",
    "val_size = float(getattr(config, 'VAL_SIZE', 0.1))  # fraction of total rows\n",
    "indices = np.arange(n)\n",
    "\n",
    "if split_by_trip:\n",
    "    trips = df['trip_id'].astype(str).unique()\n",
    "    trainval_trips, test_trips = train_test_split(trips, test_size=test_size, random_state=random_state)\n",
    "    val_rel = val_size / max(1e-12, (1.0 - test_size))\n",
    "    val_rel = min(max(val_rel, 0.0), 0.5)\n",
    "    train_trips, val_trips = train_test_split(trainval_trips, test_size=val_rel, random_state=random_state)\n",
    "\n",
    "    train_mask = df['trip_id'].astype(str).isin(train_trips).to_numpy()\n",
    "    val_mask = df['trip_id'].astype(str).isin(val_trips).to_numpy()\n",
    "    test_mask = df['trip_id'].astype(str).isin(test_trips).to_numpy()\n",
    "\n",
    "    train_idx = indices[train_mask]\n",
    "    val_idx = indices[val_mask]\n",
    "    test_idx = indices[test_mask]\n",
    "else:\n",
    "    trainval_idx, test_idx = train_test_split(indices, test_size=test_size, random_state=random_state)\n",
    "    val_rel = val_size / max(1e-12, (1.0 - test_size))\n",
    "    val_rel = min(max(val_rel, 0.0), 0.5)\n",
    "    train_idx, val_idx = train_test_split(trainval_idx, test_size=val_rel, random_state=random_state)\n",
    "\n",
    "logger.info(f\"Split sizes | train={len(train_idx):,} val={len(val_idx):,} test={len(test_idx):,} | split_by_trip={split_by_trip}\")\n",
    "\n",
    "class RowIndexDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, row_indices: np.ndarray):\n",
    "        self.row_indices = row_indices\n",
    "    def __len__(self):\n",
    "        return int(self.row_indices.shape[0])\n",
    "    def __getitem__(self, i: int):\n",
    "        return int(self.row_indices[i])\n",
    "\n",
    "def make_window_batch(row_indices: list[int]):\n",
    "    # Builds (B, seq_len, num_feat) lazily for this batch only.\n",
    "    batch_size = len(row_indices)\n",
    "    seq = np.zeros((batch_size, seq_len, num_feat), dtype=np.float32)\n",
    "    lengths = np.empty((batch_size,), dtype=np.int64)\n",
    "    stop_idx = np.empty((batch_size,), dtype=np.int64)\n",
    "    y = np.empty((batch_size, 1), dtype=np.float32)\n",
    "\n",
    "    for b, r in enumerate(row_indices):\n",
    "        pos = int(pos_in_trip[r])\n",
    "        L = seq_len if pos + 1 >= seq_len else (pos + 1)\n",
    "        start = r - L + 1\n",
    "        seq[b, -L:, :] = X_ctx_base[start : r + 1]\n",
    "        lengths[b] = L\n",
    "        stop_idx[b] = stop_idx_base[r]\n",
    "        y[b, 0] = y_base[r, 0]\n",
    "\n",
    "    return (\n",
    "        torch.from_numpy(seq),\n",
    "        torch.from_numpy(lengths),\n",
    "        torch.from_numpy(stop_idx),\n",
    "        torch.from_numpy(y),\n",
    "    )\n",
    "\n",
    "train_dataset = RowIndexDataset(train_idx)\n",
    "val_dataset = RowIndexDataset(val_idx)\n",
    "test_dataset = RowIndexDataset(test_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34868288",
   "metadata": {},
   "source": [
    "## 5. Define the V4 Architecture (GATv2Conv + GRU Fusion)\n",
    "\n",
    "Model V4 has two branches:\n",
    "\n",
    "1) **Spatial branch (GATv2Conv)**: learns a stop embedding from the static stop graph.\n",
    "2) **Temporal branch (GRU)**: learns a compact state from the last $k$ steps of context.\n",
    "\n",
    "We then **fuse** these representations and regress the delay in seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3a92149b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-13 22:16:16,569 - INFO - --- Defining Model V4 (GATv2Conv + GRU) ---\n",
      "2025-12-13 22:16:16,646 - INFO - Model V4 initialized | params(trainable)=174,209 | seq_len=12 | seq_features=8\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# STEP 5: MODEL DEFINITION (V4)\n",
    "# =============================================================================\n",
    "\n",
    "logger.info('--- Defining Model V4 (GATv2Conv + GRU) ---')\n",
    "\n",
    "dropout = float(getattr(config, 'GNN_V4_DROPOUT', 0.2))\n",
    "gru_hidden = int(getattr(config, 'GNN_V4_GRU_HIDDEN', 64))\n",
    "num_seq_features = len(context_seq_cols)\n",
    "\n",
    "class ContextAwareGATv2_GRU_V4(nn.Module):\n",
    "    def __init__(self, num_node_features: int, num_seq_features: int, gru_hidden: int, dropout: float):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Spatial encoder (similar capacity to V2)\n",
    "        self.gat1 = GATv2Conv(num_node_features, 64, heads=4, dropout=dropout)\n",
    "        self.gat2 = GATv2Conv(64 * 4, 128, heads=1, concat=False, dropout=dropout)\n",
    "        \n",
    "        # Temporal encoder\n",
    "        self.gru = nn.GRU(\n",
    "            input_size=num_seq_features,\n",
    "            hidden_size=gru_hidden,\n",
    "            num_layers=1,\n",
    "            batch_first=True,\n",
    "            dropout=0.0,\n",
    "        )\n",
    "        \n",
    "        fusion_dim = 128 + gru_hidden\n",
    "        self.fc1 = nn.Linear(fusion_dim, 256)\n",
    "        self.bn1 = nn.BatchNorm1d(256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.bn2 = nn.BatchNorm1d(128)\n",
    "        self.fc3 = nn.Linear(128, 64)\n",
    "        self.fc_out = nn.Linear(64, 1)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x_nodes, edge_index, seq_batch, lengths, stop_indices):\n",
    "        # Spatial\n",
    "        x = self.gat1(x_nodes, edge_index)\n",
    "        x = F.elu(x)\n",
    "        x = self.gat2(x, edge_index)\n",
    "        x = F.elu(x)\n",
    "        stop_emb = x[stop_indices]  # [B, 128]\n",
    "        \n",
    "        # Temporal (pack padded sequence for efficiency/correctness)\n",
    "        # lengths is [B] with values in [1, seq_len]\n",
    "        lengths_cpu = lengths.detach().to('cpu')\n",
    "        packed = pack_padded_sequence(\n",
    "            seq_batch,\n",
    "            lengths=lengths_cpu,\n",
    "            batch_first=True,\n",
    "            enforce_sorted=False,\n",
    "        )\n",
    "        _, h_n = self.gru(packed)\n",
    "        temp_emb = h_n[-1]  # [B, gru_hidden]\n",
    "        \n",
    "        # Fuse + regress\n",
    "        fused = torch.cat([stop_emb, temp_emb], dim=1)\n",
    "        out = self.dropout(F.elu(self.bn1(self.fc1(fused))))\n",
    "        out = self.dropout(F.elu(self.bn2(self.fc2(out))))\n",
    "        out = self.dropout(F.elu(self.fc3(out)))\n",
    "        return self.fc_out(out)\n",
    "\n",
    "model = ContextAwareGATv2_GRU_V4(\n",
    "    num_node_features=graph_data.x.shape[1],\n",
    "    num_seq_features=num_seq_features,\n",
    "    gru_hidden=gru_hidden,\n",
    "    dropout=dropout,\n",
    ").to(device)\n",
    "\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "logger.info(f\"Model V4 initialized | params(trainable)={trainable_params:,} | seq_len={seq_len} | seq_features={num_seq_features}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "738eaefc",
   "metadata": {},
   "source": [
    "## 6. Training Loop (Val-Aware Scheduler)\n",
    "\n",
    "We use the same training philosophy as Model V2:\n",
    "- Stream batches of lightweight tensors with a `DataLoader`.\n",
    "- Keep the graph on the GPU.\n",
    "- Optimize MAE (L1 loss).\n",
    "- Step `ReduceLROnPlateau` on **validation MAE** (not training loss)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ffa09715",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-13 22:16:26,353 - INFO - --- Preparing DataLoaders + Training ---\n",
      "2025-12-13 22:16:26,354 - INFO - Hyperparams | epochs=50 batch_size=4096 lr=0.003 wd=0.0 seq_len=12 gru_hidden=64 dropout=0.2 plateau(factor=0.5, patience=3, threshold=0.0001, min_lr=1e-06, cooldown=0)\n",
      "2025-12-13 22:16:26,355 - INFO - Starting training (scheduler stepped on VAL MAE)...\n",
      "2025-12-13 22:17:10,455 - INFO - Epoch 001/50 | train_MAE=53.41s | val_MAE=51.89s | best_val=51.89s | Δ_vs_43.18s=+8.71s | LR= 0.003 | bad_epochs=0 | sched_best=51.891558944538076 | cooldown=0\n",
      "2025-12-13 22:17:52,849 - INFO - Epoch 002/50 | train_MAE=49.61s | val_MAE=44.19s | best_val=44.19s | Δ_vs_43.18s=+1.01s | LR= 0.003 | bad_epochs=0 | sched_best=44.1913623655996 | cooldown=0\n",
      "2025-12-13 22:18:36,347 - INFO - Epoch 003/50 | train_MAE=48.60s | val_MAE=43.78s | best_val=43.78s | Δ_vs_43.18s=+0.60s | LR= 0.003 | bad_epochs=0 | sched_best=43.784389552249706 | cooldown=0\n",
      "2025-12-13 22:19:22,233 - INFO - Epoch 004/50 | train_MAE=48.10s | val_MAE=44.03s | best_val=43.78s | Δ_vs_43.18s=+0.85s | LR= 0.003 | bad_epochs=1 | sched_best=43.784389552249706 | cooldown=0\n",
      "2025-12-13 22:20:07,104 - INFO - Epoch 005/50 | train_MAE=47.70s | val_MAE=43.31s | best_val=43.31s | Δ_vs_43.18s=+0.13s | LR= 0.003 | bad_epochs=0 | sched_best=43.31220434558007 | cooldown=0\n",
      "2025-12-13 22:20:51,113 - INFO - Epoch 006/50 | train_MAE=47.34s | val_MAE=43.32s | best_val=43.31s | Δ_vs_43.18s=+0.14s | LR= 0.003 | bad_epochs=1 | sched_best=43.31220434558007 | cooldown=0\n",
      "2025-12-13 22:21:34,438 - INFO - Epoch 007/50 | train_MAE=47.01s | val_MAE=46.67s | best_val=43.31s | Δ_vs_43.18s=+3.49s | LR= 0.003 | bad_epochs=2 | sched_best=43.31220434558007 | cooldown=0\n",
      "2025-12-13 22:22:18,215 - INFO - Epoch 008/50 | train_MAE=46.71s | val_MAE=43.53s | best_val=43.31s | Δ_vs_43.18s=+0.35s | LR= 0.003 | bad_epochs=3 | sched_best=43.31220434558007 | cooldown=0\n",
      "2025-12-13 22:23:03,230 - INFO - Epoch 009/50 | train_MAE=46.48s | val_MAE=41.73s | best_val=41.73s | Δ_vs_43.18s=-1.45s | LR= 0.003 | bad_epochs=0 | sched_best=41.734566196318596 | cooldown=0\n",
      "2025-12-13 22:23:45,921 - INFO - Epoch 010/50 | train_MAE=46.25s | val_MAE=45.95s | best_val=41.73s | Δ_vs_43.18s=+2.77s | LR= 0.003 | bad_epochs=1 | sched_best=41.734566196318596 | cooldown=0\n",
      "2025-12-13 22:24:30,740 - INFO - Epoch 011/50 | train_MAE=46.06s | val_MAE=41.74s | best_val=41.73s | Δ_vs_43.18s=-1.44s | LR= 0.003 | bad_epochs=2 | sched_best=41.734566196318596 | cooldown=0\n",
      "2025-12-13 22:25:14,387 - INFO - Epoch 012/50 | train_MAE=45.85s | val_MAE=43.12s | best_val=41.73s | Δ_vs_43.18s=-0.06s | LR= 0.003 | bad_epochs=3 | sched_best=41.734566196318596 | cooldown=0\n",
      "2025-12-13 22:25:58,352 - INFO - Epoch 013/50 | train_MAE=45.67s | val_MAE=40.27s | best_val=40.27s | Δ_vs_43.18s=-2.91s | LR= 0.003 | bad_epochs=0 | sched_best=40.27376449236306 | cooldown=0\n",
      "2025-12-13 22:26:42,945 - INFO - Epoch 014/50 | train_MAE=45.54s | val_MAE=41.49s | best_val=40.27s | Δ_vs_43.18s=-1.69s | LR= 0.003 | bad_epochs=1 | sched_best=40.27376449236306 | cooldown=0\n",
      "2025-12-13 22:27:24,867 - INFO - Epoch 015/50 | train_MAE=45.38s | val_MAE=42.94s | best_val=40.27s | Δ_vs_43.18s=-0.24s | LR= 0.003 | bad_epochs=2 | sched_best=40.27376449236306 | cooldown=0\n",
      "2025-12-13 22:28:07,207 - INFO - Epoch 016/50 | train_MAE=45.26s | val_MAE=44.81s | best_val=40.27s | Δ_vs_43.18s=+1.63s | LR= 0.003 | bad_epochs=3 | sched_best=40.27376449236306 | cooldown=0\n",
      "2025-12-13 22:28:50,393 - INFO - Epoch 017/50 | train_MAE=45.15s | val_MAE=40.06s | best_val=40.06s | Δ_vs_43.18s=-3.12s | LR= 0.003 | bad_epochs=0 | sched_best=40.06380089893136 | cooldown=0\n",
      "2025-12-13 22:29:33,523 - INFO - Epoch 018/50 | train_MAE=45.03s | val_MAE=40.14s | best_val=40.06s | Δ_vs_43.18s=-3.04s | LR= 0.003 | bad_epochs=1 | sched_best=40.06380089893136 | cooldown=0\n",
      "2025-12-13 22:30:16,607 - INFO - Epoch 019/50 | train_MAE=44.92s | val_MAE=40.36s | best_val=40.06s | Δ_vs_43.18s=-2.82s | LR= 0.003 | bad_epochs=2 | sched_best=40.06380089893136 | cooldown=0\n",
      "2025-12-13 22:30:59,865 - INFO - Epoch 020/50 | train_MAE=44.80s | val_MAE=41.00s | best_val=40.06s | Δ_vs_43.18s=-2.18s | LR= 0.003 | bad_epochs=3 | sched_best=40.06380089893136 | cooldown=0\n",
      "2025-12-13 22:31:42,943 - INFO - Epoch 021/50 | train_MAE=44.72s | val_MAE=39.97s | best_val=39.97s | Δ_vs_43.18s=-3.21s | LR= 0.003 | bad_epochs=0 | sched_best=39.96517328549457 | cooldown=0\n",
      "2025-12-13 22:32:26,600 - INFO - Epoch 022/50 | train_MAE=44.63s | val_MAE=39.52s | best_val=39.52s | Δ_vs_43.18s=-3.66s | LR= 0.003 | bad_epochs=0 | sched_best=39.51938922943607 | cooldown=0\n",
      "2025-12-13 22:33:10,179 - INFO - Epoch 023/50 | train_MAE=44.58s | val_MAE=40.60s | best_val=39.52s | Δ_vs_43.18s=-2.58s | LR= 0.003 | bad_epochs=1 | sched_best=39.51938922943607 | cooldown=0\n",
      "2025-12-13 22:33:53,458 - INFO - Epoch 024/50 | train_MAE=44.43s | val_MAE=39.34s | best_val=39.34s | Δ_vs_43.18s=-3.84s | LR= 0.003 | bad_epochs=0 | sched_best=39.33807815531249 | cooldown=0\n",
      "2025-12-13 22:34:36,660 - INFO - Epoch 025/50 | train_MAE=44.39s | val_MAE=39.40s | best_val=39.34s | Δ_vs_43.18s=-3.78s | LR= 0.003 | bad_epochs=1 | sched_best=39.33807815531249 | cooldown=0\n",
      "2025-12-13 22:35:19,777 - INFO - Epoch 026/50 | train_MAE=44.32s | val_MAE=38.71s | best_val=38.71s | Δ_vs_43.18s=-4.47s | LR= 0.003 | bad_epochs=0 | sched_best=38.70657517320366 | cooldown=0\n",
      "2025-12-13 22:36:03,158 - INFO - Epoch 027/50 | train_MAE=44.24s | val_MAE=38.69s | best_val=38.69s | Δ_vs_43.18s=-4.49s | LR= 0.003 | bad_epochs=0 | sched_best=38.68638883098479 | cooldown=0\n",
      "2025-12-13 22:36:46,650 - INFO - Epoch 028/50 | train_MAE=44.18s | val_MAE=38.68s | best_val=38.68s | Δ_vs_43.18s=-4.50s | LR= 0.003 | bad_epochs=0 | sched_best=38.6760563542766 | cooldown=0\n",
      "2025-12-13 22:37:29,139 - INFO - Epoch 029/50 | train_MAE=44.12s | val_MAE=38.89s | best_val=38.68s | Δ_vs_43.18s=-4.29s | LR= 0.003 | bad_epochs=1 | sched_best=38.6760563542766 | cooldown=0\n",
      "2025-12-13 22:38:11,638 - INFO - Epoch 030/50 | train_MAE=44.05s | val_MAE=38.99s | best_val=38.68s | Δ_vs_43.18s=-4.19s | LR= 0.003 | bad_epochs=2 | sched_best=38.6760563542766 | cooldown=0\n",
      "2025-12-13 22:38:54,571 - INFO - Epoch 031/50 | train_MAE=43.97s | val_MAE=38.22s | best_val=38.22s | Δ_vs_43.18s=-4.96s | LR= 0.003 | bad_epochs=0 | sched_best=38.21806643086095 | cooldown=0\n",
      "2025-12-13 22:39:38,492 - INFO - Epoch 032/50 | train_MAE=43.90s | val_MAE=39.35s | best_val=38.22s | Δ_vs_43.18s=-3.83s | LR= 0.003 | bad_epochs=1 | sched_best=38.21806643086095 | cooldown=0\n",
      "2025-12-13 22:40:23,857 - INFO - Epoch 033/50 | train_MAE=43.83s | val_MAE=39.02s | best_val=38.22s | Δ_vs_43.18s=-4.16s | LR= 0.003 | bad_epochs=2 | sched_best=38.21806643086095 | cooldown=0\n",
      "2025-12-13 22:41:08,440 - INFO - Epoch 034/50 | train_MAE=43.79s | val_MAE=38.69s | best_val=38.22s | Δ_vs_43.18s=-4.49s | LR= 0.003 | bad_epochs=3 | sched_best=38.21806643086095 | cooldown=0\n",
      "2025-12-13 22:41:52,690 - INFO - Epoch 035/50 | train_MAE=43.74s | val_MAE=38.09s | best_val=38.09s | Δ_vs_43.18s=-5.09s | LR= 0.003 | bad_epochs=0 | sched_best=38.08725610343359 | cooldown=0\n",
      "2025-12-13 22:42:36,996 - INFO - Epoch 036/50 | train_MAE=43.68s | val_MAE=38.03s | best_val=38.03s | Δ_vs_43.18s=-5.15s | LR= 0.003 | bad_epochs=0 | sched_best=38.030038520854006 | cooldown=0\n",
      "2025-12-13 22:43:20,571 - INFO - Epoch 037/50 | train_MAE=43.63s | val_MAE=37.79s | best_val=37.79s | Δ_vs_43.18s=-5.39s | LR= 0.003 | bad_epochs=0 | sched_best=37.79286401502548 | cooldown=0\n",
      "2025-12-13 22:44:04,339 - INFO - Epoch 038/50 | train_MAE=43.58s | val_MAE=38.48s | best_val=37.79s | Δ_vs_43.18s=-4.70s | LR= 0.003 | bad_epochs=1 | sched_best=37.79286401502548 | cooldown=0\n",
      "2025-12-13 22:44:47,840 - INFO - Epoch 039/50 | train_MAE=43.51s | val_MAE=37.79s | best_val=37.79s | Δ_vs_43.18s=-5.39s | LR= 0.003 | bad_epochs=2 | sched_best=37.79286401502548 | cooldown=0\n",
      "2025-12-13 22:45:30,631 - INFO - Epoch 040/50 | train_MAE=43.46s | val_MAE=37.39s | best_val=37.39s | Δ_vs_43.18s=-5.79s | LR= 0.003 | bad_epochs=0 | sched_best=37.39088070264427 | cooldown=0\n",
      "2025-12-13 22:46:14,156 - INFO - Epoch 041/50 | train_MAE=43.42s | val_MAE=39.12s | best_val=37.39s | Δ_vs_43.18s=-4.06s | LR= 0.003 | bad_epochs=1 | sched_best=37.39088070264427 | cooldown=0\n",
      "2025-12-13 22:46:57,310 - INFO - Epoch 042/50 | train_MAE=43.37s | val_MAE=38.80s | best_val=37.39s | Δ_vs_43.18s=-4.38s | LR= 0.003 | bad_epochs=2 | sched_best=37.39088070264427 | cooldown=0\n",
      "2025-12-13 22:47:40,100 - INFO - Epoch 043/50 | train_MAE=43.34s | val_MAE=37.29s | best_val=37.29s | Δ_vs_43.18s=-5.89s | LR= 0.003 | bad_epochs=0 | sched_best=37.28511635462443 | cooldown=0\n",
      "2025-12-13 22:48:22,973 - INFO - Epoch 044/50 | train_MAE=43.28s | val_MAE=37.53s | best_val=37.29s | Δ_vs_43.18s=-5.65s | LR= 0.003 | bad_epochs=1 | sched_best=37.28511635462443 | cooldown=0\n",
      "2025-12-13 22:49:08,438 - INFO - Epoch 045/50 | train_MAE=43.24s | val_MAE=38.91s | best_val=37.29s | Δ_vs_43.18s=-4.27s | LR= 0.003 | bad_epochs=2 | sched_best=37.28511635462443 | cooldown=0\n",
      "2025-12-13 22:49:53,803 - INFO - Epoch 046/50 | train_MAE=43.21s | val_MAE=37.76s | best_val=37.29s | Δ_vs_43.18s=-5.42s | LR= 0.003 | bad_epochs=3 | sched_best=37.28511635462443 | cooldown=0\n",
      "2025-12-13 22:50:39,126 - INFO - Epoch 047/50 | train_MAE=43.15s | val_MAE=37.72s | best_val=37.29s | Δ_vs_43.18s=-5.46s | LR↓ 0.0015 | bad_epochs=0 | sched_best=37.28511635462443 | cooldown=0\n",
      "2025-12-13 22:51:24,582 - INFO - Epoch 048/50 | train_MAE=42.71s | val_MAE=36.95s | best_val=36.95s | Δ_vs_43.18s=-6.23s | LR= 0.0015 | bad_epochs=0 | sched_best=36.94793224847445 | cooldown=0\n",
      "2025-12-13 22:52:09,892 - INFO - Epoch 049/50 | train_MAE=42.67s | val_MAE=37.25s | best_val=36.95s | Δ_vs_43.18s=-5.93s | LR= 0.0015 | bad_epochs=1 | sched_best=36.94793224847445 | cooldown=0\n",
      "2025-12-13 22:52:55,181 - INFO - Epoch 050/50 | train_MAE=42.62s | val_MAE=36.95s | best_val=36.95s | Δ_vs_43.18s=-6.23s | LR= 0.0015 | bad_epochs=2 | sched_best=36.94793224847445 | cooldown=0\n",
      "2025-12-13 22:52:55,182 - INFO - V4 training complete.\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# STEP 6: TRAINING (V4)\n",
    "# =============================================================================\n",
    "\n",
    "logger.info('--- Preparing DataLoaders + Training ---')\n",
    "\n",
    "batch_size = int(getattr(config, 'GNN_V4_BATCH_SIZE', 4096))\n",
    "num_epochs = int(getattr(config, 'GNN_V4_NUM_EPOCHS', 50))\n",
    "learning_rate = float(getattr(config, 'GNN_V4_LR', 0.003))\n",
    "weight_decay = float(getattr(config, 'GNN_V4_WEIGHT_DECAY', 0.0))\n",
    "num_workers = int(getattr(config, 'NUM_WORKERS', 4))\n",
    "pin_memory = bool(getattr(config, 'PIN_MEMORY', True))\n",
    "\n",
    "sched_factor = float(getattr(config, 'GNN_V4_SCHED_FACTOR', 0.5))\n",
    "sched_patience = int(getattr(config, 'GNN_V4_SCHED_PATIENCE', 3))\n",
    "sched_threshold = float(getattr(config, 'GNN_V4_SCHED_THRESHOLD', 1e-4))\n",
    "sched_min_lr = float(getattr(config, 'GNN_V4_SCHED_MIN_LR', 1e-6))\n",
    "sched_cooldown = int(getattr(config, 'GNN_V4_SCHED_COOLDOWN', 0))\n",
    "champion_mae = float(getattr(config, 'CHAMPION_MAE', 43.18))\n",
    "\n",
    "logger.info(\n",
    "    \"Hyperparams | \"\n",
    "    f\"epochs={num_epochs} batch_size={batch_size} lr={learning_rate} wd={weight_decay} \"\n",
    "    f\"seq_len={seq_len} gru_hidden={gru_hidden} dropout={dropout} \"\n",
    "    f\"plateau(factor={sched_factor}, patience={sched_patience}, threshold={sched_threshold}, min_lr={sched_min_lr}, cooldown={sched_cooldown})\"\n",
    " )\n",
    "\n",
    "# DataLoaders build windows lazily via make_window_batch from Step 4\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=pin_memory,\n",
    "    drop_last=False,\n",
    "    collate_fn=make_window_batch,\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=pin_memory,\n",
    "    drop_last=False,\n",
    "    collate_fn=make_window_batch,\n",
    ")\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "scheduler = ReduceLROnPlateau(\n",
    "    optimizer,\n",
    "    mode='min',\n",
    "    factor=sched_factor,\n",
    "    patience=sched_patience,\n",
    "    threshold=sched_threshold,\n",
    "    cooldown=sched_cooldown,\n",
    "    min_lr=sched_min_lr,\n",
    ")\n",
    "criterion = nn.L1Loss()\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_mae(loader) -> float:\n",
    "    model.eval()\n",
    "    total = 0.0\n",
    "    n_batches = 0\n",
    "    for seq_batch, lengths, stop_idx, y_batch in loader:\n",
    "        seq_batch = seq_batch.to(device, non_blocking=True)\n",
    "        lengths = lengths.to(device, non_blocking=True)\n",
    "        stop_idx = stop_idx.to(device, non_blocking=True)\n",
    "        y_batch = y_batch.to(device, non_blocking=True)\n",
    "        out = model(graph_data.x, graph_data.edge_index, seq_batch, lengths, stop_idx)\n",
    "        loss = criterion(out, y_batch)\n",
    "        total += float(loss.item())\n",
    "        n_batches += 1\n",
    "    return total / max(1, n_batches)\n",
    "\n",
    "logger.info('Starting training (scheduler stepped on VAL MAE)...')\n",
    "best_val = float('inf')\n",
    "history = {'train_mae': [], 'val_mae': [], 'lr': []}\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_train = 0.0\n",
    "    n_train_batches = 0\n",
    "    for seq_batch, lengths, stop_idx, y_batch in train_loader:\n",
    "        seq_batch = seq_batch.to(device, non_blocking=True)\n",
    "        lengths = lengths.to(device, non_blocking=True)\n",
    "        stop_idx = stop_idx.to(device, non_blocking=True)\n",
    "        y_batch = y_batch.to(device, non_blocking=True)\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        out = model(graph_data.x, graph_data.edge_index, seq_batch, lengths, stop_idx)\n",
    "        loss = criterion(out, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_train += float(loss.item())\n",
    "        n_train_batches += 1\n",
    "\n",
    "    train_mae = total_train / max(1, n_train_batches)\n",
    "    val_mae = evaluate_mae(val_loader)\n",
    "\n",
    "    lr_before = float(optimizer.param_groups[0]['lr'])\n",
    "    scheduler.step(val_mae)\n",
    "    lr_after = float(optimizer.param_groups[0]['lr'])\n",
    "    lr_note = 'LR↓' if lr_after < lr_before else 'LR='\n",
    "    improved = val_mae < (best_val - sched_threshold)\n",
    "    if improved:\n",
    "        best_val = val_mae\n",
    "\n",
    "    sched_best = getattr(scheduler, 'best', None)\n",
    "    bad_epochs = getattr(scheduler, 'num_bad_epochs', None)\n",
    "    cooldown = getattr(scheduler, 'cooldown_counter', None)\n",
    "    delta_to_champion = val_mae - champion_mae\n",
    "\n",
    "    history['train_mae'].append(train_mae)\n",
    "    history['val_mae'].append(val_mae)\n",
    "    history['lr'].append(lr_after)\n",
    "\n",
    "    logger.info(\n",
    "        f\"Epoch {epoch+1:03d}/{num_epochs} | \"\n",
    "        f\"train_MAE={train_mae:.2f}s | val_MAE={val_mae:.2f}s | best_val={best_val:.2f}s | \"\n",
    "        f\"Δ_vs_{champion_mae:.2f}s={delta_to_champion:+.2f}s | \"\n",
    "        f\"{lr_note} {lr_after:.6g} | bad_epochs={bad_epochs} | sched_best={sched_best} | cooldown={cooldown}\"\n",
    "    )\n",
    "\n",
    "logger.info('V4 training complete.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a126a4da",
   "metadata": {},
   "source": [
    "## 7. Evaluation + Diagnostics Plot\n",
    "\n",
    "We evaluate on the held-out test set and save a diagnostic figure under `plots/` using an accurate filename:\n",
    "- `gnn_model_v4_diagnostics.png`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b3921266",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-13 22:53:21,504 - INFO - --- Starting Model V4 Evaluation on Test Set ---\n",
      "2025-12-13 22:53:21,505 - INFO - Running inference...\n",
      "2025-12-13 22:53:26,797 - INFO - --- FINAL GNN MODEL V4 RESULTS ---\n",
      "2025-12-13 22:53:26,798 - INFO - MAE (Mean Absolute Error): 36.88 seconds\n",
      "2025-12-13 22:53:26,798 - INFO - RMSE (Root Mean Sq Error): 86.23 seconds\n",
      "2025-12-13 22:53:26,799 - INFO - R² Score:                  0.8700\n",
      "2025-12-13 22:53:26,799 - INFO - VS Context RF (43.18s):  +6.30s difference\n",
      "2025-12-13 22:53:27,459 - INFO - Saved diagnostic plots to: /home/fustli/Documents/Uni/DeepL/End-of-trip_delay_prediction/plots/gnn_model_v4_diagnostics.png\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# STEP 7: EVALUATION (V4)\n",
    "# =============================================================================\n",
    "\n",
    "logger.info('--- Starting Model V4 Evaluation on Test Set ---')\n",
    "\n",
    "eval_batch_size = int(getattr(config, 'GNN_V4_EVAL_BATCH_SIZE', getattr(config, 'GNN_V4_BATCH_SIZE', 4096)))\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=eval_batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=pin_memory,\n",
    "    collate_fn=make_window_batch,\n",
    ")\n",
    "\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_targets = []\n",
    "\n",
    "logger.info('Running inference...')\n",
    "with torch.no_grad():\n",
    "    for seq_batch, lengths, stop_idx, y_batch in test_loader:\n",
    "        seq_batch = seq_batch.to(device, non_blocking=True)\n",
    "        lengths = lengths.to(device, non_blocking=True)\n",
    "        stop_idx = stop_idx.to(device, non_blocking=True)\n",
    "        out = model(graph_data.x, graph_data.edge_index, seq_batch, lengths, stop_idx)\n",
    "        all_preds.append(out.detach().cpu().numpy())\n",
    "        all_targets.append(y_batch.detach().cpu().numpy())\n",
    "\n",
    "y_pred = np.vstack(all_preds).flatten()\n",
    "y_true = np.vstack(all_targets).flatten()\n",
    "\n",
    "mae = mean_absolute_error(y_true, y_pred)\n",
    "rmse = float(np.sqrt(mean_squared_error(y_true, y_pred)))\n",
    "r2 = float(r2_score(y_true, y_pred))\n",
    "\n",
    "logger.info('--- FINAL GNN MODEL V4 RESULTS ---')\n",
    "logger.info(f\"MAE (Mean Absolute Error): {mae:.2f} seconds\")\n",
    "logger.info(f\"RMSE (Root Mean Sq Error): {rmse:.2f} seconds\")\n",
    "logger.info(f\"R² Score:                  {r2:.4f}\")\n",
    "\n",
    "champion_mae = float(getattr(config, 'CHAMPION_MAE', 43.18))\n",
    "logger.info(f\"VS Context RF ({champion_mae:.2f}s):  {champion_mae - mae:+.2f}s difference\")\n",
    "\n",
    "# Diagnostics\n",
    "fig, axes = plt.subplots(1, 3, figsize=(20, 6))\n",
    "plot_n = min(5000, len(y_pred))\n",
    "plot_indices = np.random.choice(len(y_pred), size=plot_n, replace=False)\n",
    "y_pred_sub = y_pred[plot_indices]\n",
    "y_true_sub = y_true[plot_indices]\n",
    "residuals = y_true_sub - y_pred_sub\n",
    "\n",
    "axes[0].scatter(y_true_sub, y_pred_sub, alpha=0.3, s=10, color='blue')\n",
    "axes[0].plot([y_true.min(), y_true.max()], [y_true.min(), y_true.max()], 'r--', lw=2, label='Perfect Fit')\n",
    "axes[0].set_title(f\"GNN Model V4: Predicted vs Actual\\nMAE: {mae:.2f}s | R²: {r2:.2f}\")\n",
    "axes[0].set_xlabel('Actual Delay (s)')\n",
    "axes[0].set_ylabel('Predicted Delay (s)')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].scatter(y_pred_sub, residuals, alpha=0.3, s=10, color='purple')\n",
    "axes[1].axhline(0, color='red', linestyle='--', lw=2)\n",
    "axes[1].set_title('Residuals vs Predictions')\n",
    "axes[1].set_xlabel('Predicted Delay (s)')\n",
    "axes[1].set_ylabel('Error (s)')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "sns.histplot(residuals, bins=50, kde=True, ax=axes[2], color='green')\n",
    "axes[2].axvline(0, color='red', linestyle='--', lw=2)\n",
    "axes[2].set_title('Error Distribution')\n",
    "axes[2].set_xlabel('Error (s)')\n",
    "\n",
    "fig.tight_layout()\n",
    "plot_path = os.path.join(plots_dir, 'gnn_model_v4_diagnostics.png')\n",
    "fig.savefig(plot_path, dpi=150)\n",
    "plt.close(fig)\n",
    "logger.info(f\"Saved diagnostic plots to: {plot_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebca7ea3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-13 23:05:40,892 - INFO - Exported V4 artifacts to: /home/fustli/Documents/Uni/DeepL/End-of-trip_delay_prediction/notebook/models/gnn_v4\n",
      "Exported V4 artifacts to: /home/fustli/Documents/Uni/DeepL/End-of-trip_delay_prediction/notebook/models/gnn_v4\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# STEP 8: EXPORT V4 ARTIFACTS (for FastAPI inference)\n",
    "# =============================================================================\n",
    "\n",
    "# Exports everything the web app needs to run V4 inference consistently:\n",
    "# - model weights\n",
    "# - stop_id -> node index mapping\n",
    "# - graph tensors (x, edge_index)\n",
    "# - scalers for dynamic features\n",
    "\n",
    "import os\n",
    "import joblib\n",
    "\n",
    "# Find repo root (so artifacts land in <repo>/models/gnn_v4 even if cwd is /notebook)\n",
    "_candidate = os.getcwd()\n",
    "repo_root = _candidate\n",
    "for _ in range(5):\n",
    "    if os.path.isdir(os.path.join(_candidate, 'data')) and os.path.isdir(os.path.join(_candidate, 'app')):\n",
    "        repo_root = _candidate\n",
    "        break\n",
    "    parent = os.path.abspath(os.path.join(_candidate, '..'))\n",
    "    if parent == _candidate:\n",
    "        break\n",
    "    _candidate = parent\n",
    "\n",
    "artifacts_dir = os.path.join(repo_root, 'models', 'gnn_v4')\n",
    "os.makedirs(artifacts_dir, exist_ok=True)\n",
    "\n",
    "# 1) Model weights\n",
    "model_path = os.path.join(artifacts_dir, 'model.pt')\n",
    "torch.save(model.state_dict(), model_path)\n",
    "\n",
    "# 2) Graph tensors (CPU)\n",
    "graph_path = os.path.join(artifacts_dir, 'graph.pt')\n",
    "torch.save({'x': graph_data.x.detach().cpu(), 'edge_index': graph_data.edge_index.detach().cpu()}, graph_path)\n",
    "\n",
    "# 3) Stop mapping (string stop_id -> int stop_idx)\n",
    "stop_id_to_idx = {str(stop_id): int(idx) for idx, stop_id in enumerate(stop_encoder.classes_)}\n",
    "joblib.dump(stop_id_to_idx, os.path.join(artifacts_dir, 'stop_id_to_idx.joblib'))\n",
    "\n",
    "# 4) Scalers used by the temporal features\n",
    "joblib.dump({\n",
    "    'scaler_lag': scaler_lag,\n",
    "    'scaler_roll': scaler_roll,\n",
    "    'scaler_delta': scaler_delta,\n",
    "    'scaler_prog': scaler_prog,\n",
    "}, os.path.join(artifacts_dir, 'scalers.joblib'))\n",
    "\n",
    "# 5) Meta config required to rebuild the exact architecture + feature order\n",
    "joblib.dump({\n",
    "    'seq_len': int(seq_len),\n",
    "    'context_seq_cols': list(context_seq_cols),\n",
    "    'gru_hidden': int(gru_hidden),\n",
    "    'dropout': float(dropout),\n",
    "}, os.path.join(artifacts_dir, 'meta.joblib'))\n",
    "\n",
    "logger.info(f\"Exported V4 artifacts to: {artifacts_dir}\")\n",
    "print(f\"Exported V4 artifacts to: {artifacts_dir}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
