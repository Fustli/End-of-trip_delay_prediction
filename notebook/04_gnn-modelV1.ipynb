{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36bcf4fd",
   "metadata": {},
   "source": [
    "# GNN Model V1 (Spatial baseline)\n",
    "\n",
    "## STEP 0: Environment setup\n",
    "Import libraries, load config, and initialize logging for a reproducible training run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42bf2265",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import gc\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "# GNN & PyTorch Geometric\n",
    "import torch_geometric\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "# Machine Learning utils\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "\n",
    "# Add src/ directory to path for config + logger utils\n",
    "sys.path.append(os.path.abspath('../src'))\n",
    "import config\n",
    "from utils import setup_logger\n",
    "\n",
    "logger = setup_logger(\n",
    "    name=\"gnn-model-v1\",\n",
    "    log_dir=getattr(config, 'LOG_DIR', os.path.join(os.getcwd(), 'log')),\n",
    "    filename='gnn_model_v1_training.log',\n",
    "    level=logging.INFO,\n",
    "    mode='w',\n",
    ")\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "logger.info(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "409faecc",
   "metadata": {},
   "source": [
    "## STEP 1: Data loading and feature engineering\n",
    "Load the cleaned dataset and build the node/trip features used by the spatial GNN baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d49523ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Load Data ---\n",
    "clean_path = getattr(config, 'CLEANED_CSV_PATH', os.path.join(config.DATA_DIR, \"vehicle_positions_cleaned.csv\"))\n",
    "\n",
    "if not os.path.exists(clean_path):\n",
    "    logger.error(f\"Cleaned data not found at {clean_path}\")\n",
    "    raise FileNotFoundError(\"Cleaned data not found. Please run the Cleaning Notebook first.\")\n",
    "\n",
    "logger.info(f\"Loading data from: {clean_path}\")\n",
    "df = pd.read_csv(clean_path)\n",
    "\n",
    "# --- 2. Extract Time Features ---\n",
    "# Convert string timestamp to datetime objects\n",
    "df['dt'] = pd.to_datetime(df['timestamp'])\n",
    "df['hour'] = df['dt'].dt.hour\n",
    "df['minute'] = df['dt'].dt.minute\n",
    "df['day_of_week'] = df['dt'].dt.dayofweek\n",
    "logger.info(\"Time features (hour, minute, day_of_week) extracted.\")\n",
    "\n",
    "# --- 3. Target Encoding (Historical Mean Delay) ---\n",
    "# NOTE: We are calculating this on the WHOLE dataset for graph construction simplicity.\n",
    "# In a strict production environment, this should be calculated on Train only to avoid leakage.\n",
    "logger.info(\"Calculating Global Historical Mean Delays (Target Encoding)...\")\n",
    "\n",
    "# Verify column name (sometimes it is 'stop_id', sometimes 'last_stop_id')\n",
    "stop_col = 'last_stop_id' if 'last_stop_id' in df.columns else 'stop_id'\n",
    "logger.info(f\"Using column '{stop_col}' as the Stop Identifier.\")\n",
    "\n",
    "stop_history = df.groupby(stop_col)['delay_seconds'].mean()\n",
    "global_mean = df['delay_seconds'].mean()\n",
    "\n",
    "# Map history to a new column\n",
    "df['history_mean'] = df[stop_col].map(stop_history).fillna(global_mean)\n",
    "\n",
    "# --- 4. Scaling (StandardScaler) ---\n",
    "# Neural Networks require normalized inputs (mean=0, std=1)\n",
    "features_to_scale = ['latitude', 'longitude', 'hour', 'minute', 'day_of_week', 'history_mean']\n",
    "logger.info(f\"Scaling features: {features_to_scale}\")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(df[features_to_scale])\n",
    "\n",
    "# Add these scaled features back to DF with distinct names\n",
    "df[['lat_scaled', 'lon_scaled', 'h_scaled', 'm_scaled', 'd_scaled', 'hist_scaled']] = X_scaled\n",
    "\n",
    "logger.info(f\"Data Loaded & Processed. Total Rows: {len(df):,}\")\n",
    "logger.info(f\"Feature Engineering Complete. Ready for Graph Construction.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06914811",
   "metadata": {},
   "source": [
    "## STEP 2: Graph construction\n",
    "Construct a stop graph (nodes=stops, edges=observed stop-to-stop transitions) for PyTorch Geometric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b59982",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# STEP 3: GRAPH CONSTRUCTION\n",
    "# =============================================================================\n",
    "\n",
    "logger.info(\"--- Constructing the Transit Graph ---\")\n",
    "\n",
    "# --- 1. Define Nodes (Unique Stops) ---\n",
    "# Map string IDs (e.g., \"F02165\") to integer indices (0, 1, 2...)\n",
    "# We use the LabelEncoder to create a consistent mapping\n",
    "stop_encoder = LabelEncoder()\n",
    "\n",
    "# Ensure we use the same column name identified in Step 2\n",
    "stop_col = 'last_stop_id' if 'last_stop_id' in df.columns else 'stop_id'\n",
    "df['stop_idx'] = stop_encoder.fit_transform(df[stop_col])\n",
    "\n",
    "num_nodes = len(stop_encoder.classes_)\n",
    "logger.info(f\"Total Unique Stops (Nodes): {num_nodes:,}\")\n",
    "\n",
    "# --- 2. Create Node Features Tensor [Num_Nodes, 3] ---\n",
    "# Features: Latitude, Longitude, Historical Mean Delay (All Scaled)\n",
    "# We aggregate by stop_idx to get one feature vector per stop.\n",
    "# Since Lat/Lon are static per stop, 'mean' effectively just selects the value.\n",
    "node_features_df = df.groupby('stop_idx')[['lat_scaled', 'lon_scaled', 'hist_scaled']].mean()\n",
    "\n",
    "# Convert to Float Tensor (GNNs require Float32)\n",
    "x = torch.tensor(node_features_df.values, dtype=torch.float)\n",
    "logger.info(f\"Node Features Tensor created: {x.shape}\")\n",
    "\n",
    "# --- 3. Create Edge Index (The Road Network) ---\n",
    "# Sort by trip and time to determine the sequence A -> B\n",
    "logger.info(\"Inferring edges from trip sequences...\")\n",
    "df_sorted = df.sort_values(by=['trip_id', 'timestamp'])\n",
    "\n",
    "# Shift the 'stop_idx' to find the next stop for every row within the same trip\n",
    "df_sorted['next_stop_idx'] = df_sorted.groupby('trip_id')['stop_idx'].shift(-1)\n",
    "\n",
    "# Filter valid transitions (drop the last stop of every trip where next is NaN)\n",
    "edges_df = df_sorted.dropna(subset=['next_stop_idx'])\n",
    "\n",
    "# Keep only unique connections to define the static graph structure\n",
    "unique_edges = edges_df[['stop_idx', 'next_stop_idx']].drop_duplicates()\n",
    "\n",
    "# Convert to Long Tensor [2, Num_Edges] and transpose to shape [2, E]\n",
    "# Note: We cast to integer because 'next_stop_idx' became float due to NaNs\n",
    "edge_index = torch.tensor(unique_edges.values.T, dtype=torch.long)\n",
    "\n",
    "logger.info(f\"Total Unique Connections (Edges): {edge_index.shape[1]:,}\")\n",
    "\n",
    "# --- 4. Package into Graph Object & Move to GPU ---\n",
    "# We create the PyTorch Geometric Data object\n",
    "graph_data = Data(x=x, edge_index=edge_index)\n",
    "\n",
    "# Transfer to GPU (RTX 4060)\n",
    "graph_data = graph_data.to(device)\n",
    "\n",
    "logger.info(\"Graph successfully created and loaded to GPU!\")\n",
    "logger.info(f\"Graph Info: {graph_data}\")\n",
    "\n",
    "# --- 5. Graph Sanity Check ---\n",
    "# Calculate Average Degree (Avg connections per stop)\n",
    "avg_degree = graph_data.num_edges / graph_data.num_nodes\n",
    "logger.info(f\"Average Degree: {avg_degree:.2f} (Avg connections per stop)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1031300",
   "metadata": {},
   "source": [
    "## STEP 3: Model definition\n",
    "Define a spatial GCN that combines stop embeddings with per-observation trip/time features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c17c9734",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# STEP 4: DEFINING THE MODEL ARCHITECTURE\n",
    "# =============================================================================\n",
    "\n",
    "logger.info(\"--- Defining GNN Architecture ---\")\n",
    "\n",
    "class TransitGNN(torch.nn.Module):\n",
    "    def __init__(self, num_node_features, num_trip_features):\n",
    "        super(TransitGNN, self).__init__()\n",
    "        \n",
    "        # 1. Graph Layers (Spatial Logic - \"The City Structure\")\n",
    "        self.conv1 = GCNConv(num_node_features, 64)\n",
    "        self.conv2 = GCNConv(64, 32)\n",
    "        \n",
    "        # 2. Regression Layers (Temporal Logic - \"The Specific Trip\")\n",
    "        self.fc1 = torch.nn.Linear(32 + num_trip_features, 64)\n",
    "        self.fc2 = torch.nn.Linear(64, 32)\n",
    "        self.fc3 = torch.nn.Linear(32, 1)\n",
    "\n",
    "    def forward(self, x, edge_index, trip_features, stop_indices):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        dropout_p = float(getattr(config, 'DROPOUT_RATE', 0.1))\n",
    "        x = F.dropout(x, p=dropout_p, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        batch_node_embeddings = x[stop_indices]\n",
    "        combined = torch.cat([batch_node_embeddings, trip_features], dim=1)\n",
    "        out = self.fc1(combined)\n",
    "        out = F.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        out = F.relu(out)\n",
    "        out = self.fc3(out)\n",
    "        return out\n",
    "\n",
    "num_node_features = graph_data.x.shape[1]\n",
    "num_trip_features = 3\n",
    "\n",
    "logger.info(f\"Model Input Params - Node Feats: {num_node_features}, Trip Feats: {num_trip_features}\")\n",
    "\n",
    "model = TransitGNN(num_node_features, num_trip_features).to(device)\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "logger.info(f\"Model initialized on {device} | trainable={trainable_params:,} total={total_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e993ef1f",
   "metadata": {},
   "source": [
    "## STEP 4: Training\n",
    "Prepare DataLoaders and train the model using MAE (L1 loss). Log per-epoch train MAE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7295d05a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.optim as optim\n",
    "\n",
    "# =============================================================================\n",
    "# STEP 4: TRAINING THE GNN (CONFIG-DRIVEN)\n",
    "# =============================================================================\n",
    "\n",
    "logger.info(\"--- Preparing Data Loaders ---\")\n",
    "\n",
    "trip_cols = ['h_scaled', 'm_scaled', 'd_scaled']\n",
    "num_trip_features = len(trip_cols)\n",
    "\n",
    "test_size = float(getattr(config, 'TEST_SIZE', 0.2))\n",
    "random_state = getattr(config, 'RANDOM_STATE', 42)\n",
    "batch_size = int(getattr(config, 'GNN_V1_BATCH_SIZE', getattr(config, 'BATCH_SIZE', 16384)))\n",
    "num_workers = int(getattr(config, 'NUM_WORKERS', 4))\n",
    "pin_memory = bool(getattr(config, 'PIN_MEMORY', True))\n",
    "\n",
    "X_trip = df[trip_cols].values\n",
    "X_stop_idx = df['stop_idx'].values\n",
    "y_target = df['delay_seconds'].values\n",
    "\n",
    "indices = np.arange(len(df))\n",
    "train_idx, test_idx = train_test_split(indices, test_size=test_size, random_state=random_state)\n",
    "\n",
    "logger.info(f\"Train rows: {len(train_idx):,} | Test rows: {len(test_idx):,}\")\n",
    "\n",
    "train_trip_feats = torch.tensor(X_trip[train_idx], dtype=torch.float32)\n",
    "train_stop_indices = torch.tensor(X_stop_idx[train_idx], dtype=torch.long)\n",
    "y_train_tensor = torch.tensor(y_target[train_idx], dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "test_trip_feats = torch.tensor(X_trip[test_idx], dtype=torch.float32)\n",
    "test_stop_indices = torch.tensor(X_stop_idx[test_idx], dtype=torch.long)\n",
    "y_test_tensor = torch.tensor(y_target[test_idx], dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "train_dataset = TensorDataset(train_trip_feats, train_stop_indices, y_train_tensor)\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=pin_memory,\n",
    "    drop_last=False,\n",
    " )\n",
    "\n",
    "logger.info(f\"Batch size: {batch_size} | train batches/epoch: {len(train_loader)}\")\n",
    "\n",
    "# (Re)initialize model to ensure a clean run\n",
    "model = TransitGNN(num_node_features=num_node_features, num_trip_features=num_trip_features).to(device)\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "logger.info(f\"Model parameters | trainable={trainable_params:,} total={total_params:,}\")\n",
    "\n",
    "lr = float(getattr(config, 'GNN_V1_LR', getattr(config, 'LEARNING_RATE', 0.005)))\n",
    "num_epochs = int(getattr(config, 'GNN_V1_NUM_EPOCHS', getattr(config, 'NUM_EPOCHS', 20)))\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "criterion = torch.nn.L1Loss()\n",
    "\n",
    "logger.info(f\"Training config: epochs={num_epochs} lr={lr} loss=L1(MAE)\")\n",
    "\n",
    "train_losses = []\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    batch_loss = 0.0\n",
    "    for trip_batch, stop_idx_batch, y_batch in train_loader:\n",
    "        trip_batch = trip_batch.to(device, non_blocking=True)\n",
    "        stop_idx_batch = stop_idx_batch.to(device, non_blocking=True)\n",
    "        y_batch = y_batch.to(device, non_blocking=True)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        out = model(graph_data.x, graph_data.edge_index, trip_batch, stop_idx_batch)\n",
    "        loss = criterion(out, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        batch_loss += float(loss.item())\n",
    "    avg_loss = batch_loss / max(1, len(train_loader))\n",
    "    train_losses.append(avg_loss)\n",
    "    logger.info(f\"Epoch {epoch + 1}/{num_epochs} | Train MAE: {avg_loss:.2f} sec\")\n",
    "\n",
    "logger.info(\"Training complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a5e704",
   "metadata": {},
   "source": [
    "## STEP 5: Evaluation and diagnostics\n",
    "Evaluate on the test split (MAE/RMSE/R²) and save a diagnostics figure under `plots/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d528380",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "# =============================================================================\n",
    "# STEP 5: MODEL EVALUATION & PLOTTING\n",
    "# =============================================================================\n",
    "\n",
    "logger.info(\"--- Evaluating on test set ---\")\n",
    "\n",
    "batch_size = int(getattr(config, 'GNN_V1_BATCH_SIZE', getattr(config, 'BATCH_SIZE', 16384)))\n",
    "num_workers = int(getattr(config, 'NUM_WORKERS', 4))\n",
    "pin_memory = bool(getattr(config, 'PIN_MEMORY', True))\n",
    "plots_dir = os.path.abspath(getattr(config, 'PLOTS_DIR', os.path.join(os.getcwd(), 'plots')))\n",
    "os.makedirs(plots_dir, exist_ok=True)\n",
    "\n",
    "test_dataset = TensorDataset(test_trip_feats, test_stop_indices, y_test_tensor)\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=pin_memory,\n",
    " )\n",
    "\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_targets = []\n",
    "\n",
    "logger.info(\"Running inference on test set...\")\n",
    "with torch.no_grad():\n",
    "    for trip_batch, stop_idx_batch, y_batch in test_loader:\n",
    "        trip_batch = trip_batch.to(device, non_blocking=True)\n",
    "        stop_idx_batch = stop_idx_batch.to(device, non_blocking=True)\n",
    "        out = model(graph_data.x, graph_data.edge_index, trip_batch, stop_idx_batch)\n",
    "        all_preds.append(out.cpu().numpy())\n",
    "        all_targets.append(y_batch.cpu().numpy())\n",
    "\n",
    "y_pred = np.vstack(all_preds).flatten()\n",
    "y_true = np.vstack(all_targets).flatten()\n",
    "\n",
    "mae = mean_absolute_error(y_true, y_pred)\n",
    "rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "r2 = r2_score(y_true, y_pred)\n",
    "\n",
    "logger.info(\"--- GNN MODEL V1 RESULTS ---\")\n",
    "logger.info(f\"MAE (sec):  {mae:.2f}\")\n",
    "logger.info(f\"RMSE (sec): {rmse:.2f}\")\n",
    "logger.info(f\"R²:         {r2:.4f}\")\n",
    "\n",
    "champion_mae = float(getattr(config, 'CHAMPION_MAE', 43.18))\n",
    "logger.info(f\"Delta vs champion MAE ({champion_mae:.2f}): {champion_mae - mae:+.2f}\")\n",
    "\n",
    "logger.info(\"Generating diagnostics plot...\")\n",
    "fig, axes = plt.subplots(1, 3, figsize=(20, 6))\n",
    "\n",
    "rng = np.random.RandomState(getattr(config, 'RANDOM_STATE', 42))\n",
    "plot_indices = rng.choice(len(y_pred), size=min(5000, len(y_pred)), replace=False)\n",
    "y_pred_sub = y_pred[plot_indices]\n",
    "y_true_sub = y_true[plot_indices]\n",
    "residuals = y_true_sub - y_pred_sub\n",
    "\n",
    "axes[0].scatter(y_true_sub, y_pred_sub, alpha=0.3, s=10, color='blue')\n",
    "axes[0].plot([y_true.min(), y_true.max()], [y_true.min(), y_true.max()], 'r--', lw=2, label='Perfect Fit')\n",
    "axes[0].set_title(f\"Predicted vs Actual\\nMAE: {mae:.1f}s | R²: {r2:.2f}\")\n",
    "axes[0].set_xlabel(\"Actual Delay (s)\")\n",
    "axes[0].set_ylabel(\"Predicted Delay (s)\")\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].scatter(y_pred_sub, residuals, alpha=0.3, s=10, color='purple')\n",
    "axes[1].axhline(0, color='red', linestyle='--', lw=2)\n",
    "axes[1].set_title(\"Residuals vs Predictions\")\n",
    "axes[1].set_xlabel(\"Predicted Delay (s)\")\n",
    "axes[1].set_ylabel(\"Error (Actual - Predicted)\")\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "sns.histplot(residuals, bins=50, kde=True, ax=axes[2], color='green')\n",
    "axes[2].axvline(0, color='red', linestyle='--', lw=2)\n",
    "axes[2].set_title(\"Error Distribution\")\n",
    "axes[2].set_xlabel(\"Prediction Error (s)\")\n",
    "axes[2].set_ylabel(\"Count\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plot_path = os.path.join(plots_dir, 'gnn_model_v1_diagnostics.png')\n",
    "fig.savefig(plot_path, dpi=150, bbox_inches='tight')\n",
    "plt.close(fig)\n",
    "logger.info(f\"Saved plot: {plot_path}\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
